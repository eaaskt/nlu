{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated CapsnetsS2I train + test\n",
    "\n",
    "First, import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0823 15:07:54.017735 4693904832 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import model_s2i\n",
    "import data_loader\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load word vectors and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "WORD_VEC_SCENARIO = \"FastText-300-verbsDatasets-synAntVSP-dia\"\n",
    "SCENARIO_NAME = 'no-rerouting-3-attention'\n",
    "\n",
    "word2vec_path = '../../romanian_word_vecs/cleaned-vectors.vec'\n",
    "\n",
    "# No diacritics data\n",
    "training_data_paths = [\n",
    "#         '../data-capsnets/scenario0/train.txt',\n",
    "#         '../data-capsnets/scenario1/train.txt',\n",
    "#         '../data-capsnets/scenario2/train.txt',\n",
    "#         '../data-capsnets/scenario3.1/train.txt',\n",
    "#         '../data-capsnets/scenario3.2/train.txt',\n",
    "#         '../data-capsnets/scenario3.3/train.txt',\n",
    "        '../data-capsnets/diacritics/scenario0/train.txt',\n",
    "        '../data-capsnets/diacritics/scenario1/train.txt',\n",
    "        '../data-capsnets/diacritics/scenario2/train.txt',\n",
    "        '../data-capsnets/diacritics/scenario31/train.txt',\n",
    "        '../data-capsnets/diacritics/scenario32/train.txt',\n",
    "        '../data-capsnets/diacritics/scenario33/train.txt',\n",
    "        \n",
    "    ]\n",
    "test_data_paths = [\n",
    "#     '../data-capsnets/scenario0/test.txt',\n",
    "#     '../data-capsnets/scenario1/test.txt',\n",
    "#     '../data-capsnets/scenario2/test.txt',\n",
    "#     '../data-capsnets/scenario3.1/test.txt',\n",
    "#     '../data-capsnets/scenario3.2/test.txt',\n",
    "#     '../data-capsnets/scenario3.3/test.txt',\n",
    "    '../data-capsnets/diacritics/scenario0/test.txt',\n",
    "    '../data-capsnets/diacritics/scenario1/test.txt',\n",
    "    '../data-capsnets/diacritics/scenario2/test.txt',\n",
    "    '../data-capsnets/diacritics/scenario31/test.txt',\n",
    "    '../data-capsnets/diacritics/scenario32/test.txt',\n",
    "    '../data-capsnets/diacritics/scenario33/test.txt',\n",
    "]\n",
    "\n",
    "scenario_nums = [\n",
    "#     '0_' + WORD_VEC_SCENARIO,\n",
    "#     '1_' + WORD_VEC_SCENARIO,\n",
    "#     '2_' + WORD_VEC_SCENARIO,\n",
    "#     '31_' + WORD_VEC_SCENARIO,\n",
    "#     '32_' + WORD_VEC_SCENARIO,\n",
    "#     '33_' + WORD_VEC_SCENARIO,\n",
    "#     '0_dia_' + WORD_VEC_SCENARIO,\n",
    "#     '1_dia_' + WORD_VEC_SCENARIO,\n",
    "#     '2_dia_' + WORD_VEC_SCENARIO,\n",
    "#     '31_dia_' + WORD_VEC_SCENARIO,\n",
    "#     '32_dia_' + WORD_VEC_SCENARIO,\n",
    "#     '33_dia_' + WORD_VEC_SCENARIO,\n",
    "    '0-' + SCENARIO_NAME,\n",
    "    '1-' + SCENARIO_NAME,\n",
    "    '2-' + SCENARIO_NAME,\n",
    "    '31-' + SCENARIO_NAME,\n",
    "    '32-' + SCENARIO_NAME,\n",
    "    '33-' + SCENARIO_NAME,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "------------------load word2vec begin-------------------\n",
      "loading time took 000.09\n",
      "------------------load word2vec end---------------------\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('------------------load word2vec begin-------------------')\n",
    "w2v = data_loader.load_w2v(word2vec_path)\n",
    "print('------------------load word2vec end---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag setting functions + utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def set_flags(data, scenario):\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "    tf.app.flags.DEFINE_boolean('save_model', False, 'save model to disk')\n",
    "    tf.app.flags.DEFINE_string('summaries_dir', './logs', 'tensorboard summaries')\n",
    "    tf.app.flags.DEFINE_string('ckpt_dir', './saved_models/', 'check point dir')\n",
    "    tf.app.flags.DEFINE_string('scenario_num', scenario, 'Scenario number')\n",
    "    tf.app.flags.DEFINE_string('errors_dir', './errors/', 'Errors dir')\n",
    "    tf.app.flags.DEFINE_string('results_dir', './results/', 'Results dir')\n",
    "    tf.app.flags.DEFINE_string('hyperparams_dir', './hyperparams/', 'Hyperparams dir')\n",
    "    \n",
    "    vocab_size, word_emb_size = data['embedding'].shape\n",
    "    _, max_sentence_length = data['x_tr'].shape\n",
    "    intents_number = len(data['intents_dict'])\n",
    "    slots_number = len(data['slots_dict'])\n",
    "    hidden_size = 64\n",
    "\n",
    "    tf.app.flags.DEFINE_float('keep_prob', 1, 'embedding dropout keep rate for training')\n",
    "    tf.app.flags.DEFINE_integer('hidden_size', hidden_size, 'embedding vector size')\n",
    "    tf.app.flags.DEFINE_integer('batch_size', 32, 'batch size')\n",
    "    tf.app.flags.DEFINE_integer('num_epochs', 20, 'num of epochs')\n",
    "    tf.app.flags.DEFINE_integer('vocab_size', vocab_size, 'vocab size of word vectors')\n",
    "    tf.app.flags.DEFINE_integer('max_sentence_length', max_sentence_length, 'max number of words in one sentence')\n",
    "    tf.app.flags.DEFINE_integer('intents_nr', intents_number, 'intents_number')  #\n",
    "    tf.app.flags.DEFINE_integer('slots_nr', slots_number, 'slots_number')  #\n",
    "    tf.app.flags.DEFINE_integer('word_emb_size', word_emb_size, 'embedding size of word vectors')\n",
    "    tf.app.flags.DEFINE_boolean('use_embedding', True, 'whether to use embedding or not.')\n",
    "    tf.app.flags.DEFINE_float('learning_rate', 0.01, 'learning rate')\n",
    "    tf.app.flags.DEFINE_integer('slot_routing_num', 2, 'slot routing num')\n",
    "    tf.app.flags.DEFINE_integer('intent_routing_num', 3, 'intent routing num')\n",
    "    tf.app.flags.DEFINE_integer('intent_output_dim', 16, 'intent output dimension')\n",
    "    tf.app.flags.DEFINE_integer('slot_output_dim', 2 * hidden_size, 'slot output dimension')\n",
    "    tf.app.flags.DEFINE_integer('d_a', 20, 'self attention weight hidden units number')\n",
    "    tf.app.flags.DEFINE_integer('r', 5, 'number of self attention heads')\n",
    "    tf.app.flags.DEFINE_float('alpha', 0.0001, 'coefficient for self attention loss')\n",
    "    tf.app.flags.DEFINE_integer('n_splits', 3, 'Number of cross-validation splits')\n",
    "    tf.app.flags.DEFINE_float('rerouting_coef', 0.5, 'coefficient for rerouting')\n",
    "    tf.app.flags.DEFINE_boolean('use_rerouting', False, 'whether to use rerouting or not')\n",
    "    \n",
    "    tf.app.flags.DEFINE_string('f', '', 'kernel') # Without this there's an error: unknown command line flag 'f'\n",
    "    \n",
    "    return FLAGS\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "        \n",
    "def write_csv(run_results):\n",
    "    with open('results/results_{}.csv'.format(SCENARIO_NAME), 'w') as f:\n",
    "        header_line = ['Scenario', 'Intent F1', 'Slot F1']\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header_line)\n",
    "        for k, v in run_results.items():\n",
    "            scenario_num = [k]\n",
    "            intent_score = [v['intent_f1']]\n",
    "            slot_score = [v['slot_f1']]\n",
    "            l = scenario_num + intent_score + slot_score\n",
    "            writer.writerow(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c86c3cad90b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# In case it's needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdel_all_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FLAGS' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'FLAGS' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "# In case it's needed\n",
    "del_all_flags(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import train\n",
    "import test\n",
    "\n",
    "results = dict()\n",
    "for i in range(len(training_data_paths)):\n",
    "    \n",
    "    # Load data\n",
    "    data = data_loader.read_datasets(w2v, training_data_paths[i], test_data_paths[i], test=True)\n",
    "    \n",
    "    FLAGS = set_flags(data, scenario=scenario_nums[i])\n",
    "    \n",
    "    # Train model\n",
    "    print('TRAINING ' + scenario_nums[i])\n",
    "    train.train(model_s2i.CapsNetS2I, data, FLAGS, batches_rand=False)\n",
    "    \n",
    "    # Test model\n",
    "    print('TESTING ' + scenario_nums[i])\n",
    "    intent_f, slot_f = test.test(model_s2i.CapsNetS2I, data, FLAGS)\n",
    "    results[scenario_nums[i]] = dict()\n",
    "    results[scenario_nums[i]]['intent_f1'] = intent_f\n",
    "    results[scenario_nums[i]]['slot_f1'] = slot_f\n",
    "        \n",
    "    # Reset flags\n",
    "    del_all_flags(FLAGS)\n",
    "\n",
    "write_csv(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}